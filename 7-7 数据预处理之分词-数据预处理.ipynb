{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'首先准备数据到tensorflow可以读取的格式'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''首先准备数据到tensorflow可以读取的格式'''\n",
    "'''\n",
    "1.分词\n",
    "2.词语转化成ID去表示：embedding，词语长度转化为向量\n",
    "    定义一个矩阵，第一个维度是词语长度|V|，诶二个维度是embedding_size\n",
    "3.需要索引：\n",
    "        词语A-->id(5)词语对应的embedding是哪一行的数据\n",
    "4.为了达到上述目标需要将词表统计出来   \n",
    "\n",
    "5.label统计出来，也变成ID-->\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入文件\n",
    "train_file = './cnews_data/cnews.train.txt'\n",
    "val_file = './cnews_data/cnews.val.txt'\n",
    "test_file = './cnews_data/cnews.test.txt'\n",
    "\n",
    "#输出文件  \n",
    "#分词后文件\n",
    "seg_train_file = './cnews_data/cnews.train.seg.txt'\n",
    "seg_val_file = './cnews_data/cnews.val.seg.txt'\n",
    "seg_test_file = './cnews_data/cnews.test.seg.txt'\n",
    "\n",
    "#此表文件：词语到ID的转化的映射\n",
    "vocab_file = './cnews_data/cnews.covab.txt'\n",
    "#label到ID的映射\n",
    "category_file = './cnews_data/cnews.category.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/4d/vrf7sljd17q74bxt50lvhxsw0000gn/T/jieba.cache\n",
      "Loading model cost 0.629 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def generate_seg_file(input_file,output_seg_file):\n",
    "    '''分词函数：按行对input——file的内容部分进行分词'''\n",
    "    with open(input_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_seg_file,'w') as f :\n",
    "        for line in lines:\n",
    "            label , content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content) \n",
    "            word_content = ' '\n",
    "            for word in word_iter:#出现空格被分成一个字符\n",
    "                word = word.strip()\n",
    "                if word != '':\n",
    "                    word_content += word + '   '\n",
    "            out_line = '%s\\t%s\\n'%(label, word_content.strip('  '))#将最后的多余空格删除\n",
    "            f.write(out_line)    \n",
    "\n",
    "generate_seg_file(train_file, seg_train_file)\n",
    "generate_seg_file(val_file, seg_val_file)\n",
    "generate_seg_file(test_file, seg_test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''词表构建，把分词后的结果文件统计出来生成词表'''\n",
    "def generate_vocab_file(input_seg_file, output_vocab_file):\n",
    "    with open(input_seg_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    word_dict={} #用字典来保存词语的频次信息\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split():\n",
    "            word_dict.setdefault(word, 0)#设计默认词频，如果这个词语不在字典中就默认为0，如果有的话这个setdefault函数就不会起作用\n",
    "            word_dict[word] += 1\n",
    "    #[(word,frequence),(   ,    )]\n",
    "    sorted_word_dict = sorted(word_dict.items(), key=lambda x : x[1], reverse=True )\n",
    "    with open(output_vocab_file, 'w') as f:\n",
    "        f.write('<UNW>\\t10000\\n')#在测试集中可能有些词在训练集中并没有导致找不到这个时候就返回UNK的id，在词频过滤中有些词被过滤掉也找不到。为了应付这种情况\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%d\\n' % (item[0], item[1]))\n",
    "    \n",
    "    \n",
    "\n",
    "generate_vocab_file(seg_train_file,vocab_file)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t5000\n",
      "娱乐\t5000\n",
      "家居\t5000\n",
      "房产\t5000\n",
      "教育\t5000\n",
      "时尚\t5000\n",
      "时政\t5000\n",
      "游戏\t5000\n",
      "科技\t5000\n",
      "财经\t5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'共10类，每类5000个样本，共5万个样本'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''把出现一次的词给删除掉。毕竟神经网络模型是一个概率模型。只出现一次没什么帮助'''\n",
    "def generate_category_dict(input_file, category_file):\n",
    "    with open(input_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    category_dict = {}\n",
    "    word_dict={} #用字典来保存词语的频次信息\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')    \n",
    "        category_dict.setdefault(label, 0)\n",
    "        category_dict[label] += 1\n",
    "    category_number = len(category_dict)\n",
    "    with open(category_file,'w') as f:\n",
    "        for category in category_dict:\n",
    "            line = '%s\\n' % category\n",
    "            print('%s\\t%d' % (category, category_dict[category]))\n",
    "            f.write(line)\n",
    "generate_category_dict(train_file, category_file)\n",
    "'''共10类，每类5000个样本，共5万个样本'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'预处理结束，正式进入 实现LSTM实现文本分类模型的环节'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''预处理结束，正式进入 实现LSTM实现文本分类模型的环节'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
